{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "##Copyright    O., Hamel et S. Lamari for The C00L07UN100120180002 Project.##########################\n",
    "#####################################################################################################\n",
    "\n",
    "#this file contains the code relating to the training of the EDAM paraphrase generation model (Arabic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ6cmb2cwUC0"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "TRAIN_DATA_FILE = '/content/drive/My Drive/ArabicDataset/ArabicDataSet.txt'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "file=open(TRAIN_DATA_FILE,\"r\",encoding=\"utf8\")\n",
    "test=csv.reader(file)\n",
    "i = -1\n",
    "\n",
    "for row in test:\n",
    "    sentence = ''.join(map(str, row))  \n",
    "    sentence = sentence.replace(u'أ',u'ا')\n",
    "    sentence = sentence.replace(u'إ', u'ا')\n",
    "    sentence = sentence.replace(u'ٳ', u'ا')\n",
    "    sentence = sentence.replace(u'آ', u'ا')\n",
    "    sentence = sentence.replace(u'ة', u'ه')\n",
    "    sentence = sentence.replace('-', '')\n",
    "    sentence = sentence.replace('/', '')\n",
    "    sentence = sentence.replace('\\\\', '')\n",
    "    sentence = sentence.replace('.', '')\n",
    "    sentence = sentence.replace(',', '')\n",
    "    sentence = sentence.replace(';', '')\n",
    "    sentence = sentence.replace(\"'\", '')\n",
    "    sentence = sentence.replace(\"،\", '')\n",
    "    sentence = sentence.replace('?', '')\n",
    "    sentence = sentence.replace('!', '')\n",
    "    sentence = sentence.replace(\"’\", '')\n",
    "    sentence = sentence.replace(\"؟\", '')\n",
    "    sentence = sentence.replace(\"=\", '')\n",
    "    sentence = sentence.replace(\"+\", '')\n",
    "    sentence = sentence.replace(\"'\", '')\n",
    "    sentence = sentence.replace(\"\\\"\", '')\n",
    "    sentence = sentence.replace('(', '')\n",
    "    sentence = sentence.replace(\")\", '')\n",
    "    sentence = sentence.replace(\"؟\", '')\n",
    "    sentence = sentence.replace(\"=\", '')\n",
    "    sentence = sentence.replace(\"+\", '')\n",
    "    sentence = sentence.replace(\"'\", '')\n",
    "    sentence = sentence.replace(\"\\\"\", '')\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "    if i%2==0 :\n",
    "        x.append(sentence)\n",
    "    else :\n",
    "        y.append(sentence)\n",
    "    \n",
    "#Remove sentences with length >30\n",
    "\n",
    "data = [(e1,e2) for e1,e2 in zip(x,y)] \n",
    "\n",
    "print(len(data))\n",
    "\n",
    "texts_1 = [] # list of lists\n",
    "texts_2 = [] # list of lists\n",
    "\n",
    "for (e1,e2) in data : \n",
    "\n",
    "  j1 = e1.split()\n",
    "  j2 = e2.split()\n",
    "\n",
    "  if len(j1) <= 30 and len(j2) <= 30 :\n",
    "\n",
    "    texts_1.append(e1)\n",
    "    texts_2.append(e2)\n",
    "    \n",
    "\n",
    "print(len(texts_1))\n",
    "print(len(texts_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "#PreTreatment\n",
    "\n",
    "def preTreatement(w):\n",
    "  \n",
    "  w = w.strip()\n",
    "\n",
    "  # unicode to ascii\n",
    "  w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "  # make spaces between the words and the ponctuations\n",
    "  \n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "\n",
    "  # add the start and end tokens\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# making the pre treatement to the sentences, and return the dataset in the format : phrases, praphrases\n",
    "def extractSentences():\n",
    "\n",
    "  paraphrase = [preTreatement(w) for w in texts_1]\n",
    "  phrase = [preTreatement(w) for w in texts_2]\n",
    "\n",
    "\n",
    "  return phrase, paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "# Load the dataset + Tokenization for the inputs and outputs\n",
    "\n",
    "def loadSentences():\n",
    "    \n",
    "  # creating cleaned input, output pairs\n",
    "  targetLang, inputLang = extractSentences()\n",
    "\n",
    "  inputTensor, inputLang_tokenizer = tokenize(inputLang)\n",
    "  targetTensor, targetLang_tokenizer = tokenize(targetLang)\n",
    "\n",
    "  return inputTensor, targetTensor, inputLang_tokenizer, targetLang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Load input and output tensors\n",
    "\n",
    "inputTensor, targetTensor, inputLang, targetLang = loadSentences()\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "maxLengthTarget, maxLengthInput = targetTensor.shape[1], inputTensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split (you must also leave another 20% aside for the test ... we have left this 20% in a separate file)\n",
    "\n",
    "inputTensor_train, inputTensor_val, targetTensor_train, targetTensor_val = train_test_split(inputTensor, targetTensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(inputTensor_train), len(targetTensor_train), len(inputTensor_val), len(targetTensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "#Convert from index to word\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT"
   },
   "outputs": [],
   "source": [
    "#show an example\n",
    "\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inputLang, inputTensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targetLang, targetTensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "#Definition of hyper Parameters\n",
    "\n",
    "bufferSize = len(inputTensor_train)\n",
    "batchSize = 64\n",
    "stepsPerEpoch = len(inputTensor_train)//batchSize\n",
    "embeddingDimension = 256\n",
    "units = 1024\n",
    "vocabInputSize = len(inputLang.word_index)+1\n",
    "vocabTargetSize = len(targetLang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputTensor_train, targetTensor_train)).shuffle(bufferSize)\n",
    "dataset = dataset.batch(batchSize, drop_remainder=True)\n",
    "\n",
    "#example of input and target batches\n",
    "\n",
    "exampleInputBatch, exampleTargetBatch = next(iter(dataset))\n",
    "exampleInputBatch.shape, exampleTargetBatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embeddingDimension, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embeddingDimension)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocabInputSize, embeddingDimension, units, batchSize)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(exampleInputBatch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "#Attention mechanism class\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    \n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k534zTHiDjQU"
   },
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batchSize, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "#Decoder class\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embeddingDimension, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embeddingDimension)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    \n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocabTargetSize, embeddingDimension, units, batchSize)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batchSize, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batchSize, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "#Define the optimizer and the loss function\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "#Checkpoints \n",
    "\n",
    "checkpoint_dir = '/content/drive/My Drive/TenAtt'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "#Training function\n",
    "\n",
    "\"\"\"\n",
    "Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "The decoder returns the predictions and the decoder hidden state.\n",
    "The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "Use teacher forcing to decide the next input to the decoder.\n",
    "Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "        \n",
    "    # return the encoder output and the decoder hidden state\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targetLang.word_index['<start>']] * batchSize, 1)\n",
    "\n",
    "    # giving the target as the next input (teacher forcing)\n",
    "    \n",
    "    for t in range(1, targ.shape[1]):\n",
    "        \n",
    "      # passing encoder output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  #calculate the loss\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  #calculate the gradients (for backpropagation + optimizer)\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [],
   "source": [
    "# Run the training \n",
    "\n",
    "# restore the latest checkpoint \n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "EPOCHS = 40 # 40 is just an example\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(stepsPerEpoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "    \n",
    "  # saving a checkpoint of the model after every epoch\n",
    "  \n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / stepsPerEpoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "  #Save loss value in a txt file after every epoch (Optional)\n",
    "  \n",
    "  loss_file=open(\"/content/drive/My Drive/loss/loss.txt\", \"a+\")\n",
    "  loss_file.write(str(total_loss / stepsPerEpoch)+'\\n')\n",
    "  loss_file.close()\n",
    "  \n",
    "  print(\"Model and loss saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# Passing to the inference\n",
    "\n",
    "# restore the latest checkpoint \n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "#Generate Paraphrases\n",
    "\n",
    "def evaluate(sentence):\n",
    "\n",
    "  sentence = preTreatement(sentence)\n",
    "\n",
    "  inputs = [inputLang.word_index[i] for i in sentence.split(' ') if i in inputLang.word_index.keys()]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=maxLengthInput, padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targetLang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(maxLengthTarget):\n",
    "        \n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targetLang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targetLang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def paraphrasing(sentence):\n",
    "    \n",
    "  result, sentence = evaluate(sentence)\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "result = paraphrasing('علقت الجلسه الساعه الواحده ظهرا واستونفت الساعه الثالثه مساء')\n",
    "print('the result is : ', result.split(' ')[0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For beam search\n",
    "\n",
    "listOfIndexes = [[0], [1], [2], [0], [1], [2], [0,1], [0,2], [1,2]]\n",
    "listOfChoice = [1,1,1,2,2,2,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vy2Z4uRT0PnV"
   },
   "outputs": [],
   "source": [
    "def evaluate2(sentence, indexes, choice):\n",
    "  attention_plot = np.zeros((maxLengthTarget, maxLengthInput))\n",
    "\n",
    "  sentence = preTreatement(sentence)\n",
    "  \n",
    "  inputs = [inputLang.word_index[i] for i in sentence.split(' ') if i in inputLang.word_index.keys()]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=maxLengthInput, padding='post')\n",
    "    \n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targetLang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(maxLengthTarget):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "    \n",
    "    if t not in indexes :\n",
    "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    \n",
    "    else:\n",
    "      arr = predictions[0].numpy().argsort()[-3:][::-1]\n",
    "      \n",
    "      predicted_id = arr[choice]\n",
    "\n",
    "    result += targetLang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targetLang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeoHPia-0DFB"
   },
   "outputs": [],
   "source": [
    "def paraphrasing2(sentence, indexes, choice):\n",
    "    \n",
    "  result, sentence = evaluate2(sentence, indexes, choice)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftdMcBdtzje4"
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "result = paraphrasing2('علقت الجلسه الساعه الواحده ظهرا واستونفت الساعه الثالثه مساء', listOfIndexes[0], listOfChoice[0])\n",
    "print('the result is : ', result.split(' ')[0:-2])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Arabic1SideTensorflow_Attention.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
